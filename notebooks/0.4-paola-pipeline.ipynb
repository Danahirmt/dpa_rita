{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "direccion = \"./../data/raw/prueba.csv\"\n",
    "df = pd.read_csv(direccion, nrows = 10000)\n",
    "df.head()\n",
    "\n",
    "df.Cancelled.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "direccion = \"./../data/raw/prueba.csv\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(direccion, header=\"true\", inferSchema=\"true\").limit(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    #Pasar a minusculas los nombres de columnas\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "\n",
    "    #Seleccionar columnas no vacias \n",
    "\n",
    "    base = df.select(df.year,df.quarter, df.month, df.dayofmonth, df.dayofweek, df.flightdate, df.reporting_airline, df.dot_id_reporting_airline, df.iata_code_reporting_airline, df.tail_number, df.flight_number_reporting_airline, df.originairportid, df.originairportseqid, df.origincitymarketid, df.origin, df.origincityname, df.originstate, df.originstatefips, df.originstatename, df.originwac, df.destairportid, df.destairportseqid, df.destcitymarketid, df.dest, df.destcityname, df.deststate, df.deststatefips, df.deststatename, df.destwac, df.crsdeptime, df.deptime, df.depdelay, df.depdelayminutes, df.depdel15, df.departuredelaygroups, df.deptimeblk, df.taxiout, df.wheelsoff, df.wheelson, df.taxiin, df.crsarrtime, df.arrtime, df.arrdelay, df.arrdelayminutes, df.arrdel15, df.arrivaldelaygroups, df.arrtimeblk, df.cancelled, df.diverted, df.crselapsedtime, df.actualelapsedtime, df.airtime, df.flights, df.distance, df.distancegroup, df.divairportlandings )\n",
    "\n",
    "    #agregar columna con clasificaci贸n de tiempo en horas de atraso del vuelo 0-1.5, 1.5-3.5,3.5-, cancelled\n",
    "\n",
    "    from pyspark.sql import functions as f\n",
    "    base = base.withColumn('rangoatrasohoras', f.when(f.col('cancelled') == 1, \"cancelled\").when(f.col('depdelayminutes') < 90, \"0-1.5\").when((f.col('depdelayminutes') > 90) & (f.col('depdelayminutes')<210), \"1.5-3.5\").otherwise(\"3.5-\"))\n",
    "\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType\n",
    "    from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "\n",
    "    #Funci贸n limpieza\n",
    "    def clean_text(c):\n",
    "        c = lower(c)\n",
    "        c = regexp_replace(c, \" \", \"_\")\n",
    "        c = f.split(c, '\\,')[0]\n",
    "        return c\n",
    "\n",
    "\n",
    "     # Aplicaci贸n de la funci贸n limpieza\n",
    "    base = base.withColumn(\"origincityname\", clean_text(col(\"origincityname\")))\n",
    "    base = base.withColumn(\"destcityname\", clean_text(col(\"destcityname\")))\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ignore_list(df, data_types):\n",
    "    from pyspark.sql.functions import countDistinct, approxCountDistinct\n",
    "    counts_summary = df.agg(*[countDistinct(c).alias(c) for c in data_types[\"StringType\"]])\n",
    "    counts_summary = counts_summary.toPandas()\n",
    "\n",
    "    import pandas as pd\n",
    "    counts = pd.Series(counts_summary.values.ravel())\n",
    "    counts.index = counts_summary.columns\n",
    "\n",
    "    sorted_vars = counts.sort_values(ascending = False)\n",
    "    ignore = list((sorted_vars[sorted_vars >100]).index)\n",
    "    return ignore\n",
    "\n",
    "def get_data_types(df):\n",
    "    from collections import defaultdict\n",
    "    data_types = defaultdict(list)\n",
    "    for entry in df.schema.fields:\n",
    "        data_types[str(entry.dataType)].append(entry.name)\n",
    "    return data_types\n",
    "\n",
    "def create_pipeline(df, ignore):\n",
    "    # Esto lo ponemos aqui para poder modificar las \n",
    "    #variables de los estimadores/transformadores\n",
    "    data_types = get_data_types(df)    \n",
    "    #--------------------------------------\n",
    "    \n",
    "    # -------------- STRING --------------\n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "\n",
    "    # -------------- DOUBLE --------------\n",
    "    numericals_double = [var for var in data_types[\"DoubleType\"] if var not in ignore]\n",
    "    numericals_double_imputed = [var + \"_imputed\" for var in numericals_double]\n",
    "\n",
    "    # -------------- INTEGERS --------------\n",
    "    from pyspark.sql.types import IntegerType, DoubleType\n",
    "    numericals_int = [var for var in data_types[\"IntegerType\"] if var not in ignore]\n",
    "    \n",
    "    for c in numericals_int:\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))\n",
    "        df = df.withColumn(c, df[c].cast(\"double\"))\n",
    "        \n",
    "    numericals_int_imputed = [var + \"_imputed\" for var in numericals_int]\n",
    "    # =======================================\n",
    "\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    ##            P I P E L I N E\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "    # ============= ONE HOT ENCODING ================\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "    stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in strings_used]\n",
    "    stage_one_hot = [OneHotEncoder(inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in strings_used]\n",
    "\n",
    "    # =============== IMPUTADORES ====================\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    stage_imputer_double = Imputer(inputCols = numericals_double, \n",
    "                                   outputCols = numericals_double_imputed) \n",
    "    stage_imputer_int = Imputer(inputCols = numericals_int, \n",
    "                                outputCols = numericals_int_imputed) \n",
    "\n",
    "    # ============= VECTOR ASESEMBLER ================\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "    features =  numericals_double_imputed \\\n",
    "              + [var + \"_one_hot\" for var in strings_used]\n",
    "    stage_assembler = VectorAssembler(inputCols = features, outputCol= \"assem_features\")\n",
    "\n",
    "    # ==================== SCALER =======================\n",
    "    from pyspark.ml.feature import StandardScaler\n",
    "    stage_scaler = StandardScaler(inputCol= stage_assembler.getOutputCol(), \n",
    "                                  outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "    # ================== PIPELINE ===================\n",
    "    from pyspark.ml import Pipeline\n",
    "    pipeline = Pipeline(stages= stage_string + stage_one_hot +          # Categorical Data\n",
    "                              [stage_imputer_double,\n",
    "                               stage_imputer_int,                       # Data Imputation\n",
    "                               stage_assembler,                         # Assembling data\n",
    "                               stage_scaler,                            # Standardize data\n",
    "                          ])\n",
    "                          \n",
    "    ## Tenemos que regesar el df porque las variables int las combierte en double\n",
    "    return  pipeline , df\n",
    "\n",
    "\n",
    "def imputa_categoricos(df, ignore):\n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "    \n",
    "    missing_data_fill = {}\n",
    "    for var in strings_used:\n",
    "        missing_data_fill[var] = \"missing\"\n",
    "\n",
    "    df = df.fillna(missing_data_fill)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(predictionAndLabels):\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "    \n",
    "    log = {}\n",
    "\n",
    "    # Show Validation Score (AUROC)\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    log['AUROC'] = \"%f\" % evaluator.evaluate(predictionAndLabels)    \n",
    "    print(\"Area under ROC = {}\".format(log['AUROC']))\n",
    "\n",
    "    # Show Validation Score (AUPR)\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
    "    log['AUPR'] = \"%f\" % evaluator.evaluate(predictionAndLabels)\n",
    "    print(\"Area under PR = {}\".format(log['AUPR']))\n",
    "\n",
    "    # Metrics\n",
    "    predictionRDD = predictionAndLabels.select(['label', 'prediction']) \\\n",
    "                            .rdd.map(lambda line: (line[1], line[0]))\n",
    "    metrics = MulticlassMetrics(predictionRDD)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "    # Overall statistics\n",
    "    log['precision'] = \"%s\" % metrics.precision()\n",
    "    log['recall'] = \"%s\" % metrics.recall()\n",
    "    log['F1 Measure'] = \"%s\" % metrics.fMeasure()\n",
    "    print(\"[Overall]\\tprecision = %s | recall = %s | F1 Measure = %s\" % \\\n",
    "            (log['precision'], log['recall'], log['F1 Measure']))\n",
    "\n",
    "    # Statistics by class\n",
    "    labels = [0.0, 1.0]\n",
    "    for label in sorted(labels):\n",
    "        log[label] = {}\n",
    "        log[label]['precision'] = \"%s\" % metrics.precision(label)\n",
    "        log[label]['recall'] = \"%s\" % metrics.recall(label)\n",
    "        log[label]['F1 Measure'] = \"%s\" % metrics.fMeasure(label, \n",
    "                                                           beta=0.5)\n",
    "        print(\"[Class %s]\\tprecision = %s | recall = %s | F1 Measure = %s\" \\\n",
    "                  % (label, log[label]['precision'], \n",
    "                    log[label]['recall'], log[label]['F1 Measure']))\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "def cv_stats(cvModel):\n",
    "    bestModel =  cvModel.bestModel\n",
    "    print(\"iter\", bestModel.stages[-1]._java_obj.getMaxIter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ids(X_train, X_test, y_train, y_test):\n",
    "    from pyspark.sql.functions import monotonically_increasing_id\n",
    "    \n",
    "    X_train = X_train.withColumn(\"id\", monotonically_increasing_id())\n",
    "    X_test = X_test.withColumn(\"id\", monotonically_increasing_id())\n",
    "    y_train = y_train.withColumn(\"id\", monotonically_increasing_id())\n",
    "    y_test = y_test.withColumn(\"id\", monotonically_increasing_id())\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(stage_pca.k, [1]) \\\n",
    ".addGrid(lr.maxIter, [1]) \\\n",
    ".build()\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt_paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(stage_pca.k, [1]) \\\n",
    ".addGrid(dt.maxDepth, [2]) \\\n",
    ".build()\n",
    "\n",
    "paramGrid_list = [lr_paramGrid, dt_paramGrid]\n",
    "model_list = [lr,dt]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crea uno contra todos manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "data_types = get_data_types(df)\n",
    "ignore =   ignore_list(df, data_types) \n",
    "illegal = [s for s in df.columns if \"del\" in s]\n",
    "extra_illegal = ['cancelled', 'rangoatrasohoras']\n",
    "legal = [var for var in df.columns if (var not in ignore and var not in illegal and var not in extra_illegal)]\n",
    "lista_objetivos = df.select('rangoatrasohoras').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "df = imputa_categoricos(df, ignore)\n",
    "X = df[legal]\n",
    "y = df[['rangoatrasohoras']]\n",
    "\n",
    "pipeline, X = create_pipeline(X, ignore)\n",
    "\n",
    "X_train, X_test = X.randomSplit([0.8,0.2], 123)\n",
    "y_train, y_test = y.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "model = pipeline.fit(X_train)\n",
    "\n",
    "X_train = model.transform(X_train)\n",
    "X_test = model.transform(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = add_ids(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objetivo:  0-1.5\n",
      "Modelo evaluado:  LogisticRegression_ec4b5c3d5156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for objetivo in lista_objetivos:\n",
    "    print(\"objetivo: \", objetivo)\n",
    "    \n",
    "    y_test = y_test.withColumn(\"label\",  when(y_test.rangoatrasohoras == objetivo, 1.0).otherwise(0.0))\n",
    "    y_train = y_train.withColumn(\"label\",  when(y_train.rangoatrasohoras == objetivo, 1.0).otherwise(0.0))\n",
    "\n",
    "    df_train = X_train.join(y_train, \"id\", \"outer\").drop(\"id\")\n",
    "    df_test = X_test.join(y_test, \"id\", \"outer\").drop(\"id\")\n",
    "    \n",
    "    from pyspark.ml.feature import PCA\n",
    "    stage_pca = PCA(k = 15,inputCol = \"scaled_features\", \n",
    "                            outputCol = \"features\")\n",
    "    \n",
    "    for clr_model, params in zip(model_list, paramGrid_list):\n",
    "        print(\"Modelo evaluado: \", clr_model)\n",
    "        pipeline = Pipeline(stages= [stage_pca, clr_model])\n",
    "\n",
    "        from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "        crossval = CrossValidator(estimator=pipeline,\n",
    "                                  estimatorParamMaps=params,\n",
    "                                  evaluator=MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                                  numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "        cvModel  = crossval.fit(df_train)\n",
    "        cv_stats(cvModel)\n",
    "        prediction = cvModel.transform(df_test)\n",
    "        evaluate(prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDIVIDUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stage_scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d221750c640c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                          fitIntercept=True) \n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m stage_pca = PCA(k = 15,inputCol = stage_scaler.getOutputCol(), \n\u001b[0m\u001b[1;32m     24\u001b[0m                 outputCol = \"features\")\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stage_scaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Separamos en train y test\n",
    "# Este dataframe ya debe filtrar\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "data_types = get_data_types(df)\n",
    "ignore =   ignore_list(df, data_types) \n",
    "\n",
    "df2 = df.withColumnRenamed(\"cancelled\",\"label\")\n",
    "df2 = imputa_categoricos(df2, ignore)\n",
    "\n",
    "# Tenemos que regesar el pipeline porque las variables int las combierte en double\n",
    "init_stages ,df2 = create_pipeline(df2, ignore)\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "clr = LogisticRegression(maxIter=10, regParam=0.01,\n",
    "                         fitIntercept=True) \n",
    "from pyspark.ml.feature import PCA\n",
    "stage_pca = PCA(k = 15,inputCol = stage_scaler.getOutputCol(), \n",
    "                outputCol = \"features\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages= init_stages + [stage_pca, clr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(stage_pca.k, [2,3]) \\\n",
    ".addGrid(clr.maxIter, [2,3]) \\\n",
    ".build()\n",
    "\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      evaluator=BinaryClassificationEvaluator(),\n",
    "                      numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "df_train, df_test = df2.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "cvModel  = crossval.fit(df_train)\n",
    "\n",
    "cv_stats(cvModel)\n",
    "\n",
    "prediction = cvModel.transform(df_test)\n",
    "\n",
    "evaluate(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://people.stat.sc.edu/haigang/sparkCaseStudy.html\n",
    "https://chih-ling-hsu.github.io/2018/09/17/spark-mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(metrics, df):\n",
    "    labels = df.rdd.map(lambda lp: lp.label).distinct().collect()\n",
    "    for label in sorted(labels):\n",
    "        print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "        print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "        print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "        print (\"\")\n",
    "\n",
    "    # Weighted stats\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n",
    "    print(\"Accuracy = %s\" % metrics.accuracy)\n",
    "\n",
    "import collections  \n",
    "\n",
    "TestResult = collections.namedtuple(\"TestResult\", [\"params\", \"metrics\"])\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "class CrossValidatorVerbose(CrossValidator):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        folds = []\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        metricName = eva.getMetricName()\n",
    "        nFolds = self.getOrDefault(self.numFolds)\n",
    "        seed = self.getOrDefault(self.seed)\n",
    "        h = 1.0 / nFolds\n",
    "\n",
    "        randCol = self.uid + \"_rand\"\n",
    "        df = dataset.select(\"*\", rand(seed).alias(randCol))\n",
    "        metrics = [0.0] * numModels\n",
    "\n",
    "        for i in range(nFolds):\n",
    "            folds.append([])\n",
    "            foldNum = i + 1\n",
    "            print(\"Comparing models on fold %d\" % foldNum)\n",
    "\n",
    "            validateLB = i * h\n",
    "            validateUB = (i + 1) * h\n",
    "            condition = (df[randCol] >= validateLB) & (df[randCol] < validateUB)\n",
    "            validation = df.filter(condition)\n",
    "            train = df.filter(~condition)\n",
    "\n",
    "            for j in range(numModels):\n",
    "                paramMap = epm[j]\n",
    "                model = est.fit(train, paramMap)\n",
    "                # TODO: duplicate evaluator to take extra params from input\n",
    "                prediction = model.transform(validation, paramMap)\n",
    "                metric = eva.evaluate(prediction)\n",
    "                metrics[j] += metric\n",
    "\n",
    "                avgSoFar = metrics[j] / foldNum\n",
    "                print(\"params: %s\\t%s: %f\\tavg: %f\" % (\n",
    "                    {param.name: val for (param, val) in paramMap.items()},\n",
    "                    metricName, metric, avgSoFar))\n",
    "\n",
    "                predictionLabels = prediction.select(\"prediction\", \"label\")\n",
    "                allMetrics = MulticlassMetrics(predictionLabels.rdd)\n",
    "                folds[i].append(TestResult(paramMap.items(), allMetrics))\n",
    "\n",
    "\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "\n",
    "        bestParams = epm[bestIndex]\n",
    "        bestModel = est.fit(dataset, bestParams)\n",
    "        avgMetrics = [m / nFolds for m in metrics]\n",
    "        bestAvg = avgMetrics[bestIndex]\n",
    "        print(\"Best model:\\nparams: %s\\t%s: %f\" % (\n",
    "            {param.name: val for (param, val) in bestParams.items()},\n",
    "            metricName, bestAvg))\n",
    "\n",
    "        return self._copyValues(CrossValidatorModel(bestModel, avgMetrics)), folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
