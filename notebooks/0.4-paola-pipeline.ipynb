{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "direccion = \"./../data/raw/prueba.csv\"\n",
    "df = pd.read_csv(direccion, nrows = 10000)\n",
    "df.head()\n",
    "\n",
    "df.Cancelled.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "direccion = \"./../data/raw/prueba.csv\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(direccion, header=\"true\", inferSchema=\"true\").limit(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    #Pasar a minusculas los nombres de columnas\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "\n",
    "    #Seleccionar columnas no vacias \n",
    "\n",
    "    base = df.select(df.year,df.quarter, df.month, df.dayofmonth, df.dayofweek, df.flightdate, df.reporting_airline, df.dot_id_reporting_airline, df.iata_code_reporting_airline, df.tail_number, df.flight_number_reporting_airline, df.originairportid, df.originairportseqid, df.origincitymarketid, df.origin, df.origincityname, df.originstate, df.originstatefips, df.originstatename, df.originwac, df.destairportid, df.destairportseqid, df.destcitymarketid, df.dest, df.destcityname, df.deststate, df.deststatefips, df.deststatename, df.destwac, df.crsdeptime, df.deptime, df.depdelay, df.depdelayminutes, df.depdel15, df.departuredelaygroups, df.deptimeblk, df.taxiout, df.wheelsoff, df.wheelson, df.taxiin, df.crsarrtime, df.arrtime, df.arrdelay, df.arrdelayminutes, df.arrdel15, df.arrivaldelaygroups, df.arrtimeblk, df.cancelled, df.diverted, df.crselapsedtime, df.actualelapsedtime, df.airtime, df.flights, df.distance, df.distancegroup, df.divairportlandings )\n",
    "\n",
    "    #agregar columna con clasificaci贸n de tiempo en horas de atraso del vuelo 0-1.5, 1.5-3.5,3.5-, cancelled\n",
    "\n",
    "    from pyspark.sql import functions as f\n",
    "    base = base.withColumn('rangoatrasohoras', f.when(f.col('cancelled') == 1, \"cancelled\").when(f.col('depdelayminutes') < 90, \"0-1.5\").when((f.col('depdelayminutes') > 90) & (f.col('depdelayminutes')<210), \"1.5-3.5\").otherwise(\"3.5-\"))\n",
    "\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType\n",
    "    from pyspark.sql.functions import col, lower, regexp_replace, split\n",
    "\n",
    "    #Funci贸n limpieza\n",
    "    def clean_text(c):\n",
    "        c = lower(c)\n",
    "        c = regexp_replace(c, \" \", \"_\")\n",
    "        c = f.split(c, '\\,')[0]\n",
    "        return c\n",
    "\n",
    "\n",
    "     # Aplicaci贸n de la funci贸n limpieza\n",
    "    base = base.withColumn(\"origincityname\", clean_text(col(\"origincityname\")))\n",
    "    base = base.withColumn(\"destcityname\", clean_text(col(\"destcityname\")))\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "def ignore_list(df, data_types):\n",
    "    from pyspark.sql.functions import countDistinct, approxCountDistinct\n",
    "    counts_summary = df.agg(*[countDistinct(c).alias(c) for c in data_types[\"StringType\"]])\n",
    "    counts_summary = counts_summary.toPandas()\n",
    "\n",
    "    import pandas as pd\n",
    "    counts = pd.Series(counts_summary.values.ravel())\n",
    "    counts.index = counts_summary.columns\n",
    "\n",
    "    sorted_vars = counts.sort_values(ascending = False)\n",
    "    ignore = list((sorted_vars[sorted_vars >100]).index)\n",
    "    return ignore\n",
    "\n",
    "def get_data_types(df):\n",
    "    from collections import defaultdict\n",
    "    data_types = defaultdict(list)\n",
    "    for entry in df.schema.fields:\n",
    "        data_types[str(entry.dataType)].append(entry.name)\n",
    "    return data_types\n",
    "\n",
    "def create_pipeline(df):\n",
    "    # Esto lo ponemos aqui para poder modificar las \n",
    "    #variables de los estimadores/transformadores\n",
    "    \n",
    "    data_types = get_data_types(df)\n",
    "    ignore =   ignore_list(df, data_types) \n",
    "        \n",
    "    #--------------------------------------\n",
    "    \n",
    "    # -------------- STRING --------------\n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "\n",
    "    # -------------- DOUBLE --------------\n",
    "    numericals_double = [var for var in data_types[\"DoubleType\"] if var not in ignore]\n",
    "    numericals_double_imputed = [var + \"_imputed\" for var in numericals_double]\n",
    "\n",
    "    # -------------- INTEGERS --------------\n",
    "    for c in data_types[\"IntegerType\"]:\n",
    "        df = df.withColumn(c+ \"_cast_to_double\", df[c].cast(\"double\"))\n",
    "\n",
    "    numericals_int = [var for var in  df.columns if var.endswith(\"_cast_to_double\")]  \n",
    "    numericals_int = [var for var in numericals_int if var not in ignore] \n",
    "    numericals_int_imputed = [var + \"_imputed\" for var in numericals_int]\n",
    "    # =======================================\n",
    "\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    ##            P I P E L I N E\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "    # ============= ONE HOT ENCODING ================\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "    stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in strings_used]\n",
    "    stage_one_hot = [OneHotEncoder(inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in strings_used]\n",
    "\n",
    "    # =============== IMPUTADORES ====================\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    stage_imputer_double = Imputer(inputCols = numericals_double, \n",
    "                                   outputCols = numericals_double_imputed) \n",
    "    stage_imputer_int = Imputer(inputCols = numericals_int, \n",
    "                                outputCols = numericals_int_imputed) \n",
    "\n",
    "    # ============= VECTOR ASESEMBLER ================\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "    features =  numericals_double_imputed \\\n",
    "              + [var + \"_one_hot\" for var in strings_used]\n",
    "    stage_assembler = VectorAssembler(inputCols = features, outputCol= \"assem_features\")\n",
    "\n",
    "    # ==================== Standariza =======================\n",
    "    from pyspark.ml.feature import StandardScaler\n",
    "    stage_scaler = StandardScaler(inputCol= stage_assembler.getOutputCol(), \n",
    "                                  outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "    # ==================== PCA =======================\n",
    "    from pyspark.ml.feature import PCA\n",
    "    stage_pca = PCA(k = 15,inputCol = stage_scaler.getOutputCol(), \n",
    "                    outputCol = \"features\")\n",
    "\n",
    "    # =================== MODELS =====================\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    clr = LogisticRegression(maxIter=10, regParam=0.01,\n",
    "                             fitIntercept=True) \n",
    "\n",
    "    # ================== PIPELINE ===================\n",
    "\n",
    "    pipeline = Pipeline(stages= stage_string + stage_one_hot +          # Categorical Data\n",
    "                              [stage_imputer_double,\n",
    "                               stage_imputer_int,                       # Data Imputation\n",
    "                               stage_assembler,                         # Assembling data\n",
    "                               stage_scaler,                            # Standardize data\n",
    "                               stage_pca,                               # Dimensionality Reduction\n",
    "                               clr\n",
    "                          ])\n",
    "    \n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(stage_pca.k, [2,3]) \\\n",
    "    .addGrid(clr.maxIter, [2,3]) \\\n",
    "    .build()\n",
    "\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "    \n",
    "    df_train, df_test = df.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "    cvModel = crossval.fit(df_train)\n",
    "    \n",
    "    # Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "    prediction = cvModel.transform(test)\n",
    "    print(prediction)\n",
    "    for row in selected:\n",
    "        print(row)\n",
    "        \n",
    "    ## Tenemos que regesar el pipeline porque las variables int las combierte en double\n",
    "    return pipeline, crossval, df\n",
    "\n",
    "\n",
    "def imputa_categoricos(df):\n",
    "    data_types = get_data_types(df)\n",
    "    ignore =   ignore_list(df, data_types) \n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "    \n",
    "    missing_data_fill = {}\n",
    "    for var in strings_used:\n",
    "        missing_data_fill[var] = \"missing\"\n",
    "\n",
    "    df = df.fillna(missing_data_fill)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en train y test\n",
    "# Este dataframe ya debe filtrar\n",
    "\n",
    "df2 = df.withColumnRenamed(\"cancelled\",\"label\")\n",
    "df2 = imputa_categoricos(df2)\n",
    "\n",
    "# Tenemos que regesar el pipeline porque las variables int las combierte en double\n",
    "pipeline, paramGrid, df2 = create_pipeline(df2)\n",
    "print(pipeline.getStages())\n",
    "model = pipeline.fit(df2)\n",
    "df2 = model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto lo ponemos aqui para poder modificar las \n",
    "#variables de los estimadores/transformadores\n",
    "\n",
    "df = df.withColumnRenamed(\"cancelled\",\"label\")\n",
    "df = imputa_categoricos(df)\n",
    "\n",
    "\n",
    "data_types = get_data_types(df)\n",
    "ignore =   ignore_list(df, data_types) \n",
    "\n",
    "#--------------------------------------\n",
    "\n",
    "# -------------- STRING --------------\n",
    "strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "\n",
    "# -------------- DOUBLE --------------\n",
    "numericals_double = [var for var in data_types[\"DoubleType\"] if var not in ignore]\n",
    "numericals_double_imputed = [var + \"_imputed\" for var in numericals_double]\n",
    "\n",
    "# -------------- INTEGERS --------------\n",
    "for c in data_types[\"IntegerType\"]:\n",
    "    df = df.withColumn(c+ \"_cast_to_double\", df[c].cast(\"double\"))\n",
    "\n",
    "numericals_int = [var for var in  df.columns if var.endswith(\"_cast_to_double\")]  \n",
    "numericals_int = [var for var in numericals_int if var not in ignore] \n",
    "numericals_int_imputed = [var + \"_imputed\" for var in numericals_int]\n",
    "# =======================================\n",
    "\n",
    "## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "##            P I P E L I N E\n",
    "## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# ============= ONE HOT ENCODING ================\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in strings_used]\n",
    "stage_one_hot = [OneHotEncoder(inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in strings_used]\n",
    "\n",
    "# =============== IMPUTADORES ====================\n",
    "from pyspark.ml.feature import Imputer\n",
    "stage_imputer_double = Imputer(inputCols = numericals_double, \n",
    "                               outputCols = numericals_double_imputed) \n",
    "stage_imputer_int = Imputer(inputCols = numericals_int, \n",
    "                            outputCols = numericals_int_imputed) \n",
    "\n",
    "# ============= VECTOR ASESEMBLER ================\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features =  numericals_double_imputed \\\n",
    "          + [var + \"_one_hot\" for var in strings_used]\n",
    "stage_assembler = VectorAssembler(inputCols = features, outputCol= \"assem_features\")\n",
    "\n",
    "# ==================== Standariza =======================\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "stage_scaler = StandardScaler(inputCol= stage_assembler.getOutputCol(), \n",
    "                              outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# ==================== PCA =======================\n",
    "from pyspark.ml.feature import PCA\n",
    "stage_pca = PCA(k = 15,inputCol = stage_scaler.getOutputCol(), \n",
    "                outputCol = \"features\")\n",
    "\n",
    "# =================== MODELS =====================\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "clr = LogisticRegression(maxIter=10, regParam=0.01,\n",
    "                         fitIntercept=True) \n",
    "\n",
    "# ================== PIPELINE ===================\n",
    "\n",
    "pipeline = Pipeline(stages= stage_string + stage_one_hot +          # Categorical Data\n",
    "                          [stage_imputer_double,\n",
    "                           stage_imputer_int,                       # Data Imputation\n",
    "                           stage_assembler,                         # Assembling data\n",
    "                           stage_scaler,                            # Standardize data\n",
    "                           stage_pca,                               # Dimensionality Reduction\n",
    "                           clr\n",
    "                      ])\n",
    "\n",
    "\n",
    "df_train, df_test = df.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "#model = pipeline.fit(df_train)\n",
    "#result = model.transform(df_test)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(stage_pca.k, [2,3]) \\\n",
    ".addGrid(clr.maxIter, [2,3]) \\\n",
    ".build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      evaluator=BinaryClassificationEvaluator(),\n",
    "                      numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "df_train, df_test = df.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "cvModel = crossval.fit(df_train)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(df_test)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
