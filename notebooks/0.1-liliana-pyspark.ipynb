{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.1596407738787475,0.8403592261212525], prediction=1.0\n",
      "(5, l m n) --> prob=[0.8378325685476744,0.16216743145232562], prediction=0.0\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976037,0.9307336686702395], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[0.9821575333444218,0.01784246665557808], prediction=0.0\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "    \n",
    "# al igual que en el ejemplo anterior, creamos un dataframe a través\n",
    "# de una lista con los datos de entrenamiento, la lista esta formada\n",
    "# por tuplas (id, texto, label). Esta forma no nos servirá para poder meterla\n",
    "# en los objetos de ML, pero más adelante arreglaremos esto\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"my_app\") \\\n",
    "            .config('spark.sql.codegen.wholeStage', False) \\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"mykey\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"mysecret\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"eu-west-3.amazonaws.com\")\n",
    "\n",
    "# IMAGINAMOS QUE LEEMOS DATOS DEL S3\n",
    "\n",
    "\n",
    "# FEATURE ENGINEERING (Transformacion de datos)\n",
    "\n",
    "# MODELLING\n",
    "\n",
    "\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "\n",
    "# Definimos los transformers: Tokenizer y HashingTF, y los \n",
    "# estimators: LogisticRegression que ocuparemos. Nota que aquí no hemos hecho\n",
    "# ningun fit todavia... la magia vendrá más adelante ;)\n",
    "# Tokenizer convierte el string de entrada (inputCol) a minúsculas y separa en\n",
    "# palabras utilizando como separador el espacio\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# HashingTF permite hashear cada palabra utilizando MurmurHash3 convirtiendo\n",
    "# el hash generado en el índice a poner en el \"TDM\". Este método optimiza el\n",
    "# tiempo para generar el TDM de TF-IDF \"normal\". Para evitar colisiones en\n",
    "# la conversión a hash se aumenta el número de buckets -se recomienda ocupar\n",
    "# potencias de 2 para balancear las cubetas-\n",
    "# Nota que en este transformer estamos ocupando como entrada la salida del\n",
    "# transformer Tokenizer\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# Ocuparemos una regresión logistica de nuex\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "# Aqui viene lo bonito... definimos un pipeline que tiene como etapas/pasos\n",
    "# primero el tokenizer, luego el hashing y luego la regresión logística. Aquí\n",
    "# estamos definiendo el flujo de procesamiento, el DAG! \n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "\n",
    "# Voila, solo se requiere de hacer fit al pipeline para que esto funcione\n",
    "# como un pipeline, siguiendo el orden de los pasos establecidos en la \n",
    "# definicion del pipeline :) ... recuerda que el fit hace \n",
    "# el entrenamiento una vez que ya definimos las configuraciones de \n",
    "# los objetos que ocuparemos (transformers y estimators)\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Creamos el dataframe de pruebas mock! -> Nota que aqui no hay \n",
    "# label!!!! (asi funcionaría en producción cierto!)\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "\n",
    "# Lixto, \"ejecutamos\" el pipeline haciendo un transform al pipeline para \n",
    "# obtener las predicciones del set de pruebas\n",
    "prediction = model.transform(test)\n",
    "\n",
    "# De nuevo, prediction es un DataFrame generado con un transformer generado\n",
    "# a través de estimadores y transformers :) \n",
    "# Seleccionamos las columnas que queremos ver \n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"({}, {}) --> prob={}, prediction={}\".format( \\\n",
    "    rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.1257, 0.8743]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9952, 0.0048]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.307, 0.693]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.804, 0.196]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(), #solo auc o areaunderPR:(\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# creamos nuestro set de datos de entrada categorico\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "# Esta funcion agrega un id numerico a cada valor diferente de un valor categorico \n",
    "# es como establecer los niveles en R de una factor pero los niveles son numericos,\n",
    "# sus id. El indice se establece por orden de frecuencia (descendente), por lo que \n",
    "# el indice 0 corresponde a la variable que aparece con mas frecuencia\n",
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = string_indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()\n",
    "\n",
    "\n",
    "# OneHotEncoder no tiene un fit ya que solo es un transformador\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Configuramos el estimator StandarScaler como lo necesitamos (por default\n",
    "# withMean esta en False porque hace que se regrese un vector dense...\n",
    "# hay que tener cuidado con eso cuando estemos manejandoo vectores sparse\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "# Creamos el modelo StandardScaler para los datos de entrada\n",
    "scaler_model = scaler.fit(data_frame)\n",
    "\n",
    "# Transformamos los datos \n",
    "scaled_data = scaler_model.transform(data_frame)\n",
    "scaled_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data_frame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "data_frame.show()\n",
    "\n",
    "# Configuramos el estimator MinMaxScaler como lo necesitamos\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Creamos el modelo MinMaxScaler (transformer)\n",
    "scaler_model = scaler.fit(data_frame)\n",
    "\n",
    "# Transformamos los datos reescalando \n",
    "scaled_data = scaler_model.transform(data_frame)\n",
    "# Nota que cuando pedimos getMin y getMax lo hacemos al estimator, no al modelo\n",
    "print(\"Features scaled to range: [{}, {}]\".format(scaler.getMin(), scaler.getMax()))\n",
    "scaled_data.select(\"features\", \"scaled_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "prediction = model.transform(test)\n",
    "\n",
    "\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"({}, {}) --> prob={}, prediction={}\".format( \\\n",
    "    rid, text, str(prob), prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'HashingTF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-af3b70f91efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mparamGrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParamGridBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHashingTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0maddGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'HashingTF'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "model_1 = lr.fit(training)\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "print(lr.extractParamMap())\n",
    "\n",
    "\n",
    "param_map = {lr.maxIter: 20}\n",
    "# Si el valor ya existe en el diccionario puedes actualizarlo\n",
    "param_map[lr.maxIter] = 30  \n",
    "# También puedes actualizar varios parámetros del diccionario al mismo tiempo\n",
    "param_map.update({lr.regParam: 0.1, lr.threshold: 0.55}) \n",
    "\n",
    "\n",
    "param_map_2 = {lr.probabilityCol: \"my_probability\"}  \n",
    "param_map_combined = param_map.copy()\n",
    "param_map_combined.update(param_map_2)\n",
    "\n",
    "model_2 = lr.fit(training, param_map_combined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "#aquí queremos ver cono que parametros se quedo configurado el modelo que ocupamos para entrenar\n",
    "print(lr.extractParamMap())\n",
    "\n",
    "\n",
    "#aqui queremos ver con qué parámetros se quedó configurado el modelo que ocupamos para entrenar\n",
    "print(lr.extractParamMap())\n",
    "\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "prediction = model_2.transform(test)\n",
    "\n",
    "type(prediction)\n",
    "prediction.columns\n",
    "\n",
    "result = prediction.select(\"features\", \"label\", \"my_probability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features={}, label={} -> prob={}, prediction={}\".format( \\\n",
    "    row.features, row.label, row.my_probability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "\n",
    "# Creamos nuestro set de entrada para formar la TDM\n",
    "sentence_data = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "# Ocupamos el transformer Tokenizer para separar por palabras\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# Aqui no hay train! porque no estamos entrenando nanda... estamos en un problema\n",
    "# de IR. Tokenizer no tiene un metodo fit -no hay entrenamiento-\n",
    "words_data = tokenizer.transform(sentence_data)\n",
    "\n",
    "# Ocupamos el transformer CountVectorizer para generar una matriz de \n",
    "# terminos y sus frecuencias \n",
    "count_vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "featurized_model = count_vectorizer.fit(words_data)\n",
    "featurized_data = featurized_model.transform(words_data)\n",
    "featurized_data.show(truncate=False)\n",
    "\n",
    "# Ocupamos IDF para obtener el IDF de la coleccion de documentos mock que \n",
    "# generamos. IDF si tiene un metodo fit a traves del cual le enviamos el set \n",
    "# de tokens al que queremos obtener el IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=1)\n",
    "# Aqui obtenemos el modelo a ocupar (transformer) a ocupar \n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)\n",
    "\n",
    "rescaled_data.select(\"label\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_parquet = spark.read.parquet(\"s3://dpa/flights/airlines/parquet/\")\n",
    "airlines_parquet.show()\n",
    "airlines_parquet.printSchema()\n",
    "airlines_parquet.count()\n",
    "\n",
    "airlines_csv.describe().show()\n",
    "\n",
    "# withColumnRenamed\n",
    "\n",
    "original = flights.columns\n",
    "new_col_names = [element.lower() for element in flights.columns]\n",
    "\n",
    "df = reduce(lambda flights, idx: flights.withColumnRenamed(original[idx], new_col_names[idx]), range(len(original)), flights)\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"vuelos\")\n",
    "spark.sql(\"select count(*) from vuelos\").show()\n",
    "sql_query = \"\"\"\n",
    "select year, month, day, airline, origin_airport, destination_airport, departure_delay\n",
    "from vuelos\n",
    "where departure_delay > 0 \n",
    "and departure_delay < 100\n",
    "\"\"\" \n",
    "\n",
    "departure_delays = spark.sql(sql_query)\n",
    "\n",
    "flights_csv.describe([\"YEAR\",\"MONTH\",\"DAY\"]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
