{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (1.12.45)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.45 in /opt/conda/lib/python3.7/site-packages (from boto3) (1.15.45)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.45->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.45->boto3) (1.25.7)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.45->boto3) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.45->boto3) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.7/site-packages (2.8.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva cubeta\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./../')\n",
    "\n",
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "#import src\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "from src.features.build_features import clean\n",
    "from src.models.train_model import get_data, evaluate\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import monotonically_increasing_id, countDistinct, approxCountDistinct, when\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, Imputer, VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./../')\n",
    "\n",
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "#import src\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from src.utils.db_utils import execute_sql\n",
    "from src.utils.s3_utils import create_bucket\n",
    "from src.models.train_model import run_model,prepare_data\n",
    "from src.models.save_model import parse_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./../')\n",
    "\n",
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "#import src\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "from src.models.train_model import get_data, run_model,prepare_data, get_models_params_dic,evaluate,add_meta_data\n",
    "from src.models.save_model import save_upload, parse_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numIt = 2\n",
    "numPCA = 1\n",
    "bucname = \"model-dpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://model-dpa/24042020_cancelled_LR_=#iter#-%2$%#pca#-%1&.zip'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objetivo = \"cancelled\"\n",
    "model_name = \"LR\"\n",
    "hyperparams = {\"iter\": int(numIt),\n",
    "                \"pca\": int(numPCA)}\n",
    "\n",
    "output_path = parse_filename(objetivo, model_name, hyperparams)\n",
    "output_path = \"s3://\" + str(bucname) +  output_path[1:] + \".model.zip\"\n",
    "\n",
    "output_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting rows from table using cursor.fetchall\n",
      "2\n",
      "Print each row and it's columns values\n",
      "('24042020', 'cancelled', 'LR', '{\"iter\": 2, \"pca\": 1}', '0.623870', '0.017011', '0.9858980702622464', '0.9858980702622464', '0.9858980702622464', '42.935035705566406', '0.2', '15958')\n",
      "('24042020', 'cancelled', 'LR', '{\"iter\": 2, \"pca\": 1}', '0.623870', '0.017011', '0.9858980702622464', '0.9858980702622464', '0.9858980702622464', '42.84245729446411', '0.2', '15958')\n",
      "PostgreSQL connection is closed\n",
      "Nueva cubeta\n",
      "models-dpa\n",
      "test-aws-boto\n",
      "/home/jovyan/work/src/models\n",
      "[StringIndexer_a2afb507a0e7, StringIndexer_d766eccd63d6, StringIndexer_4518684b31c1, StringIndexer_f498d20d1b6a, StringIndexer_3c492e302635, StringIndexer_30bb471521ab, StringIndexer_30e69d4c8cbe, StringIndexer_17b41f0e7983, OneHotEncoder_495426ba9b67, OneHotEncoder_5be587f4f4b3, OneHotEncoder_bf0ef4ce96d9, OneHotEncoder_86511d556215, OneHotEncoder_d68109e588ef, OneHotEncoder_4029f6ad0024, OneHotEncoder_b80a6054cf47, OneHotEncoder_dfd8cd807165, Imputer_a1b7abfcdcab, Imputer_9f9a3b718a80, VectorAssembler_4519fb31e7a0, StandardScaler_012d90610190]\n",
      "Modelo evaluado:  LogisticRegression_4946876831d1 con params:  aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 2)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "Area under ROC = 0.623870\n",
      "Area under PR = 0.017011\n",
      "[[3985.    0.]\n",
      " [  57.    0.]]\n",
      "[Overall]\tprecision = 0.9858980702622464 | recall = 0.9858980702622464 | F1 Measure = 0.9858980702622464\n",
      "[Class 0.0]\tprecision = 0.9858980702622464 | recall = 1.0 | F1 Measure = 0.9886865479085\n",
      "[Class 1.0]\tprecision = 0.0 | recall = 0.0 | F1 Measure = 0.0\n",
      "24042020_cancelled_LR_=#iter#-%2$%#pca#-%1&.model.zip file is created successfully!\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "objetivo = \"cancelled\"\n",
    "model = \"LR\"\n",
    "hyperparams = {\"iter\": int(numIt),\n",
    "                \"pca\": int(numPCA)}\n",
    "\n",
    "# Esta funcion ya guarda los metadatos en un BD\n",
    "# Corre los modelos y el mejor lo guarda en un S3 y guarda toda la info\n",
    "# Toma la base de semantic\n",
    "run_model(objetivo, model, hyperparams, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/src/models\n",
      "[StringIndexer_5134f6b8fde8, StringIndexer_7a84310d5566, StringIndexer_f46240e9982f, StringIndexer_f37609338763, StringIndexer_967f904a98c9, StringIndexer_6341f550810f, StringIndexer_9e99f7b8b0e5, StringIndexer_c6d49137d73a, OneHotEncoder_ab77ed42b9fa, OneHotEncoder_0ff4fdc6e344, OneHotEncoder_332d66c2d4ea, OneHotEncoder_a5726697cd40, OneHotEncoder_f8e085c55a0c, OneHotEncoder_263e5498387d, OneHotEncoder_0109a878a11d, OneHotEncoder_17b86f63c8d9, Imputer_b00167a33c03, Imputer_422bdae3ee89, VectorAssembler_52c22faeb163, StandardScaler_89fbf24c9a80]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-604fcbacb9ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mnum_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pca\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_pca\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     stage_pca = PCA(k = num_pca,inputCol = \"scaled_features\", \n\u001b[0m\u001b[1;32m     25\u001b[0m                         outputCol = \"features\")\n\u001b[1;32m     26\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, countDistinct, approxCountDistinct, when\n",
    "objetivo = \"cancelled\"\n",
    "model_name = \"LR\"\n",
    "hyperparams = {\"pca\": 1, \"iter\":1}\n",
    "test_split = 0.2\n",
    "\n",
    "df = get_data(False)\n",
    "first_stages,df = prepare_data(df)\n",
    "\n",
    "df = df.withColumn(\"label\",  when(df.rangoatrasohoras == objetivo, 1.0).otherwise(0.0))\n",
    "\n",
    "# Selecciona el modelo\n",
    "model_dic, paramGrid_dic  = get_models_params_dic()\n",
    "clr_model = model_dic[model_name]\n",
    "\n",
    "# Parametros especificos\n",
    "num_it = int(hyperparams[\"iter\"])\n",
    "if num_it > 0:\n",
    "    clr_model.setMaxIter(num_it)\n",
    "\n",
    "# Adds new stages\n",
    "num_pca = int(hyperparams[\"pca\"])\n",
    "if num_pca > 0:\n",
    "    stage_pca = PCA(k = num_pca,inputCol = \"scaled_features\", \n",
    "                        outputCol = \"features\")\n",
    "else:\n",
    "    stage_pca = PCA(k = 8,inputCol = \"scaled_features\", \n",
    "                outputCol = \"features\")\n",
    "\n",
    "# Checar que no se haya corrido este modelo \n",
    "\n",
    "print(\"Modelo evaluado: \", clr_model, \"con params: \", clr_model.explainParams())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e9b6b99a1090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creates Pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfirst_stages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstage_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclr_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtest_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_split\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creates Pipeline\n",
    "pipeline = Pipeline(stages= first_stages + [stage_pca, clr_model])\n",
    "\n",
    "df_train, df_test = df.randomSplit([(1-test_split),test_split ], 123)\n",
    "\n",
    "start = time.time()\n",
    "cvModel  = pipeline.fit(df_train)\n",
    "end = time.time()\n",
    "\n",
    "prediction = cvModel.transform(df_test)\n",
    "log = evaluate(prediction)\n",
    "\n",
    "\n",
    "#Guarda en s3\n",
    "save_upload(cvModel, objetivo, model_name, hyperparams)\n",
    "\n",
    "# Sube metadatos a RDS\n",
    "# --- Metadata -----\n",
    "train_time = end - start\n",
    "train_nrows = df_train.count()\n",
    "# -------------------\n",
    "add_meta_data(objetivo, model_name,hyperparams, log,train_time, test_split, train_nrows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputa_categoricos(df, ignore,data_types):\n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "\n",
    "    missing_data_fill = {}\n",
    "    for var in strings_used:\n",
    "        missing_data_fill[var] = \"missing\"\n",
    "\n",
    "    df = df.fillna(missing_data_fill)\n",
    "    return df\n",
    "\n",
    "def ignore_list(df, data_types):\n",
    "    counts_summary = df.agg(*[countDistinct(c).alias(c) for c in data_types[\"StringType\"]])\n",
    "    counts_summary = counts_summary.toPandas()\n",
    "\n",
    "    counts = pd.Series(counts_summary.values.ravel())\n",
    "    counts.index = counts_summary.columns\n",
    "\n",
    "    sorted_vars = counts.sort_values(ascending = False)\n",
    "    ignore = list((sorted_vars[sorted_vars >100]).index)\n",
    "    return ignore\n",
    "\n",
    "def get_data_types(df):\n",
    "    data_types = defaultdict(list)\n",
    "    for entry in df.schema.fields:\n",
    "        data_types[str(entry.dataType)].append(entry.name)\n",
    "    return data_types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(df, ignore):\n",
    "    # Esto lo ponemos aqui para poder modificar las \n",
    "    #variables de los estimadores/transformadores\n",
    "    data_types = get_data_types(df)    \n",
    "    #--------------------------------------\n",
    "    \n",
    "    # -------------- STRING --------------\n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "\n",
    "    # -------------- DOUBLE --------------\n",
    "    numericals_double = [var for var in data_types[\"DoubleType\"] if var not in ignore]\n",
    "    numericals_double_imputed = [var + \"_imputed\" for var in numericals_double]\n",
    "\n",
    "    # -------------- INTEGERS --------------\n",
    "    from pyspark.sql.types import IntegerType, DoubleType\n",
    "    numericals_int = [var for var in data_types[\"IntegerType\"] if var not in ignore]\n",
    "    \n",
    "    for c in numericals_int:\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))\n",
    "        df = df.withColumn(c, df[c].cast(\"double\"))\n",
    "        \n",
    "    numericals_int_imputed = [var + \"_imputed\" for var in numericals_int]\n",
    "    # =======================================\n",
    "\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    ##            P I P E L I N E\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "    # ============= ONE HOT ENCODING ================\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "    stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in strings_used]\n",
    "    stage_one_hot = [OneHotEncoder(inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in strings_used]\n",
    "\n",
    "    # =============== IMPUTADORES ====================\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    stage_imputer_double = Imputer(inputCols = numericals_double, \n",
    "                                   outputCols = numericals_double_imputed) \n",
    "    stage_imputer_int = Imputer(inputCols = numericals_int, \n",
    "                                outputCols = numericals_int_imputed) \n",
    "\n",
    "    # ============= VECTOR ASESEMBLER ================\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "    features =  numericals_double_imputed \\\n",
    "              + [var + \"_one_hot\" for var in strings_used]\n",
    "    stage_assembler = VectorAssembler(inputCols = features, outputCol= \"assem_features\")\n",
    "\n",
    "    # ==================== SCALER =======================\n",
    "    from pyspark.ml.feature import StandardScaler\n",
    "    stage_scaler = StandardScaler(inputCol= stage_assembler.getOutputCol(), \n",
    "                                  outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "    # ================== PIPELINE ===================\n",
    "    stages= stage_string + stage_one_hot +  [             # Categorical Data\n",
    "                               stage_imputer_double,\n",
    "                               stage_imputer_int,        # Data Imputation\n",
    "                               stage_assembler,          # Assembling data\n",
    "                               stage_scaler] \n",
    "                          \n",
    "    ## Tenemos que regesar el df porque las variables int las combierte en double\n",
    "    return  stages , df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_params_dic():\n",
    "    stage_pca = PCA(k = 15,inputCol = \"scaled_features\",\n",
    "                        outputCol = \"features\")\n",
    "\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(stage_pca.k, [1]) \\\n",
    "    .addGrid(lr.maxIter, [1]) \\\n",
    "    .build()\n",
    "\n",
    "    dt = DecisionTreeClassifier()\n",
    "\n",
    "    dt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(stage_pca.k, [1]) \\\n",
    "    .addGrid(dt.maxDepth, [2]) \\\n",
    "    .build()\n",
    "\n",
    "    paramGrid_dic= {\"LR\":lr_paramGrid,\"DT\":dt_paramGrid}\n",
    "    model_dic = {\"LR\":lr,\"DT\":dt}\n",
    "\n",
    "    return model_dic,paramGrid_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    data_types = get_data_types(df)\n",
    "    ignore =   ignore_list(df, data_types) \n",
    "    illegal = [s for s in df.columns if \"del\" in s]\n",
    "    extra_illegal = ['cancelled', 'rangoatrasohoras']\n",
    "    legal = [var for var in df.columns if (var not in ignore and var not in illegal and var not in extra_illegal)]\n",
    "    lista_objetivos = df.select('rangoatrasohoras').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "    df = imputa_categoricos(df, ignore,data_types)\n",
    "    \n",
    "    df_legal = df[legal]\n",
    "    y = df[['rangoatrasohoras']]\n",
    "    \n",
    "    df_legal = df_legal.withColumn(\"id\", monotonically_increasing_id())\n",
    "    y = y.withColumn(\"id\", monotonically_increasing_id())\n",
    "    \n",
    "    stages, df_new = create_pipeline(df_legal, ignore)\n",
    "\n",
    "    df_junto = df_new.join(y, \"id\", \"outer\").drop(\"id\")\n",
    "\n",
    "    return df_junto, stages\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def run_model(objetivo, model_name, hyperparams, luigi= False):\n",
    "    df = get_data(luigi)\n",
    "    df, first_stages = prepare_data(df)\n",
    "  \n",
    "    df = df.withColumn(\"label\",  when(df.rangoatrasohoras == objetivo, 1.0).otherwise(0.0))\n",
    "    \n",
    "    # Selecciona el modelo\n",
    "    model_dic, paramGrid_dic  = get_models_params_dic()\n",
    "    clr_model = model_dic[model_name]\n",
    "    \n",
    "    # Parametros especificos\n",
    "    num_it = int(hyperparams[\"iter\"])\n",
    "    if num_it > 0:\n",
    "        clr_model.setMaxIter(num_it)\n",
    "    \n",
    "    # Adds new stages\n",
    "    num_pca = int(hyperparams[\"pca\"])\n",
    "    if num_pca > 0:\n",
    "        stage_pca = PCA(k = num_pca,inputCol = \"scaled_features\", \n",
    "                            outputCol = \"features\")\n",
    "    else:\n",
    "        stage_pca = PCA(k = 8,inputCol = \"scaled_features\", \n",
    "                    outputCol = \"features\")\n",
    "    \n",
    "    # Checar que no se haya corrido este modelo \n",
    "    \n",
    "    print(\"Modelo evaluado: \", clr_model, \"con params: \", clr_model.explainParams())\n",
    "    \n",
    "    # Creates Pipeline\n",
    "    pipeline = Pipeline(stages= first_stages + [stage_pca, clr_model])\n",
    "\n",
    "    df_train, df_test = df.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "    cvModel  = pipeline.fit(df_train)\n",
    "    prediction = cvModel.transform(df_test)\n",
    "    evaluate(prediction)\n",
    "    \n",
    "    #Sacar metadatos\n",
    "    #Insertar metadatos\n",
    "    #Guardar modelos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/src/models\n",
      "Modelo evaluado:  LogisticRegression_da34f1229d49 con params:  aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "Area under ROC = 0.623870\n",
      "Area under PR = 0.017011\n",
      "[[3985.    0.]\n",
      " [  57.    0.]]\n",
      "[Overall]\tprecision = 0.9858980702622464 | recall = 0.9858980702622464 | F1 Measure = 0.9858980702622464\n",
      "[Class 0.0]\tprecision = 0.9858980702622464 | recall = 1.0 | F1 Measure = 0.9886865479085\n",
      "[Class 1.0]\tprecision = 0.0 | recall = 0.0 | F1 Measure = 0.0\n"
     ]
    }
   ],
   "source": [
    "objetivo = \"cancelled\"\n",
    "model = \"LR\"\n",
    "hyperparams = {\"pca\": 1, \"iter\":1}\n",
    "\n",
    "run_model(objetivo, model, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "\n",
    "objetivo = \"cancelled\"\n",
    "model_name = \"LR\"\n",
    "hyperparams = {\"pca\": 1, \"iter\":1}\n",
    "test_split = 0.2\n",
    "\n",
    "def run_model(objetivo, model_name, hyperparams, luigi= False, test_split = 0.2):\n",
    "    df = get_data(False)\n",
    "    df, first_stages = prepare_data(df)\n",
    "\n",
    "    df = df.withColumn(\"label\",  when(df.rangoatrasohoras == objetivo, 1.0).otherwise(0.0))\n",
    "\n",
    "    # Selecciona el modelo\n",
    "    model_dic, paramGrid_dic  = get_models_params_dic()\n",
    "    clr_model = model_dic[model_name]\n",
    "\n",
    "    # Parametros especificos\n",
    "    num_it = int(hyperparams[\"iter\"])\n",
    "    if num_it > 0:\n",
    "        clr_model.setMaxIter(num_it)\n",
    "\n",
    "    # Adds new stages\n",
    "    num_pca = int(hyperparams[\"pca\"])\n",
    "    if num_pca > 0:\n",
    "        stage_pca = PCA(k = num_pca,inputCol = \"scaled_features\", \n",
    "                            outputCol = \"features\")\n",
    "    else:\n",
    "        stage_pca = PCA(k = 8,inputCol = \"scaled_features\", \n",
    "                    outputCol = \"features\")\n",
    "\n",
    "    # Checar que no se haya corrido este modelo \n",
    "\n",
    "    print(\"Modelo evaluado: \", clr_model, \"con params: \", clr_model.explainParams())\n",
    "\n",
    "    # Creates Pipeline\n",
    "    pipeline = Pipeline(stages= first_stages + [stage_pca, clr_model])\n",
    "\n",
    "    df_train, df_test = df.randomSplit([(1-test_split),test_split ], 123)\n",
    "\n",
    "    start = time.time()\n",
    "    cvModel  = pipeline.fit(df_train)\n",
    "    end = time.time()\n",
    "\n",
    "    prediction = cvModel.transform(df_test)\n",
    "    log = evaluate(prediction)\n",
    "\n",
    "\n",
    "    #Guarda en s3\n",
    "    save_upload(cvModel, objetivo, model_name, hyperparams)\n",
    "\n",
    "    # Sube metadatos a RDS\n",
    "    # --- Metadata -----\n",
    "    train_time = end - start\n",
    "    train_nrows = df_train.count()\n",
    "    # -------------------\n",
    "    add_meta_data(objetivo, model_name,hyperparams, log,train_time, test_split, train_nrows)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC = log['AUROC']\n",
    "AUPR = log['AUPR']\n",
    "precision = log['precision']\n",
    "recall = log['recall']\n",
    "f1 =log['F1 Measure'] \n",
    "train_nrows = df_train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva cubeta\n"
     ]
    }
   ],
   "source": [
    "from src.models.save_model import save_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model.zip file is created successfully!\n"
     ]
    }
   ],
   "source": [
    "save_upload(cvModel, objetivo, model_name, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.8.5-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "from src.utils.db_utils import execute_sql,insert_query,show_select\n",
    "file_dir = \"./../src/utils/sql/metada_model.sql\"\n",
    "# Crear schema\n",
    "execute_sql(file_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 4, 22, 21, 28, 36, 73245)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "\n",
    "today = date.today()\n",
    "d1 = today.strftime(\"%d%m%Y\")\n",
    "d1\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from src.utils.db_utils import execute_sql,insert_query,show_select\n",
    "\n",
    "def add_meta_data(objetivo, model_name,hyperparams, log,train_time, test_split, train_nrows):\n",
    "    AUROC = log['AUROC']\n",
    "    AUPR = log['AUPR']\n",
    "    precision = log['precision']\n",
    "    recall = log['recall']\n",
    "    f1 =log['F1 Measure'] \n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%d%m%Y\")\n",
    "\n",
    "    query = \"\"\" INSERT INTO metadatos.models (fecha, objetivo, model_name, hyperparams, AUROC, AUPR, precision, recall, f1, train_time, test_split, train_nrows ) VALUES ( %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s  ) \"\"\"\n",
    "    values = (d1,\n",
    "             objetivo, model_name,\n",
    "             json.dumps(hyperparams),\n",
    "             AUROC, AUPR, precision, recall, f1, train_time, test_split, train_nrows)\n",
    "    insert_query(query, values)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting rows from table using cursor.fetchall\n",
      "1\n",
      "Print each row and it's columns values\n",
      "('22042020', 'cancelled', 'LR', '{\"pca\": 1, \"iter\": 1}', '0.623870', '0.017011', '0.9858980702622464', '0.9858980702622464', '0.9858980702622464', '-48.46292996406555', '0.2', '15958')\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "show_select(\"select * from metadatos.models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.12.43-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 701 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.16.0,>=1.15.43\n",
      "  Downloading botocore-1.15.43-py2.py3-none-any.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 74 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 47 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3) (1.25.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.43->boto3) (1.14.0)\n",
      "Installing collected packages: docutils, jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.12.43 botocore-1.15.43 docutils-0.15.2 jmespath-0.9.5 s3transfer-0.3.3\n",
      "Nueva cubeta\n",
      "models-dpa\n",
      "models-dpa\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "from src.utils.s3_utils import describe_s3, get_s3_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = cvModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from datetime import date\n",
    "\n",
    "def parse_filename(objetivo, model_name, hyperparams):    \n",
    "    para_string = json.dumps(hyperparams) \n",
    "    para_string = para_string.replace(\" \", \"%\")\n",
    "    para_string = para_string.replace('\"', \"#\")\n",
    "    para_string = para_string.replace('}', \"&\")\n",
    "    para_string = para_string.replace('{', \"=\")\n",
    "    para_string = para_string.replace(':', \"-\")\n",
    "    para_string = para_string.replace(',', \"$\")\n",
    "    \n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%d%m%Y\")\n",
    "\n",
    "    saved_model_name = \"./\" + d1 + \"_\" + objetivo + \"_\" + model_name + \"_\" + para_string \n",
    "\n",
    "    return saved_model_name\n",
    "\n",
    "\n",
    "def save_upload(cvModel, objetivo, model_name, hyperparams,bucket_name = \"models-dpa\"):\n",
    "    trained_model = cvModel.stages[-1]\n",
    "    \n",
    "    saved_model_name = parse_filename(objetivo, model_name, hyperparams) + \".model\"\n",
    "    key_name = saved_model_name[2:]\n",
    "    \n",
    "    # Save model\n",
    "    trained_model.save(saved_model_name)\n",
    "\n",
    "    # Zip model \n",
    "    zip_model(key_name)\n",
    "    \n",
    "    new_saved_model = saved_model_name +\".zip\"\n",
    "    new_key_name = new_saved_model[2:] \n",
    "\n",
    "    # Upload file\n",
    "    upload_file_to_bucket(new_saved_model, bucket_name, new_key_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_and_zip(cvModel, objetivo, model_name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_name = parse_filename(objetivo, model_name, hyperparams)\n",
    "saved_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from src import (\n",
    "    BUCKET,\n",
    "    MY_REGION,\n",
    "    MY_REGION2,\n",
    "    MY_PROFILE,\n",
    "    MY_KEY,\n",
    "    MY_AMI ,\n",
    "    MY_VPC ,\n",
    "    MY_GATEWAY,\n",
    "    MY_SUBNET,\n",
    "    MY_GROUP\n",
    ")\n",
    "\n",
    "\n",
    "ses = boto3.session.Session(profile_name=MY_PROFILE, region_name=MY_REGION,)\n",
    "s3 = ses.resource('s3')\n",
    "bucket_name = BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model.zip file is created successfully!\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"models-dpa\"\n",
    "#my_bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "model_dir = parse_filename(objetivo, model_name, hyperparams)\n",
    "key_name = saved_model_name[2:]\n",
    "\n",
    "zip_model(key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    " \n",
    "#Declare the function to return all file paths of the particular directory\n",
    "def retrieve_file_paths(dirName):\n",
    "    # setup file paths variable\n",
    "    filePaths = []\n",
    "    # Read all directory, subdirectories and file lists\n",
    "    for root, directories, files in os.walk(dirName):\n",
    "        for filename in files:\n",
    "            # Create the full filepath by using os module.\n",
    "            filePath = os.path.join(root, filename)\n",
    "            filePaths.append(filePath)\n",
    "    # return all paths\n",
    "    return filePaths\n",
    "  \n",
    "def zip_model(dir_name):\n",
    "    #https://linuxhint.com/python_zip_file_directory/\n",
    "    # Call the function to retrieve all files and folders of the assigned directory\n",
    "    filePaths = retrieve_file_paths(dir_name)\n",
    "    # writing files to a zipfile\n",
    "    zip_file = zipfile.ZipFile(dir_name+'.zip', 'w')\n",
    "    with zip_file:\n",
    "        # writing each file one by one\n",
    "        for file in filePaths:\n",
    "            zip_file.write(file)\n",
    "    print(dir_name+'.zip file is created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"models-dpa\"\n",
    "#my_bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "model_dir = parse_filename(objetivo, model_name, hyperparams) +\".zip\"\n",
    "key_name = saved_model_name[2:] \n",
    "\n",
    "ses = boto3.session.Session(profile_name=MY_PROFILE, region_name=MY_REGION,)\n",
    "s3 = ses.resource('s3')\n",
    "\n",
    "# Escribimos el archivo al bucket, usando el binario\n",
    "s3.meta.client.upload_file(model_dir, bucket_name, key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save(saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel, LogisticRegressionModel\n",
    "import os \n",
    "\n",
    "prueba = LogisticRegressionModel.load(saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_2e27ecb108eb, numClasses = 2, numFeatures = 1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CURRENT_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1 = 22042020\n"
     ]
    }
   ],
   "source": [
    "# dd/mm/YY\n",
    "today = date.today()\n",
    "d1 = today.strftime(\"%d%m%Y\")\n",
    "print(\"d1 =\", d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
