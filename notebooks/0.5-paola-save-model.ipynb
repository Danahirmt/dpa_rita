{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./../')\n",
    "\n",
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "#import src\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "from src.features.build_features import clean\n",
    "from src.models.train_model import get_data, evaluate\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.sql.functions import monotonically_increasing_id, countDistinct, approxCountDistinct, when\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, Imputer, VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputa_categoricos(df, ignore,data_types):\n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "\n",
    "    missing_data_fill = {}\n",
    "    for var in strings_used:\n",
    "        missing_data_fill[var] = \"missing\"\n",
    "\n",
    "    df = df.fillna(missing_data_fill)\n",
    "    return df\n",
    "\n",
    "def ignore_list(df, data_types):\n",
    "    counts_summary = df.agg(*[countDistinct(c).alias(c) for c in data_types[\"StringType\"]])\n",
    "    counts_summary = counts_summary.toPandas()\n",
    "\n",
    "    counts = pd.Series(counts_summary.values.ravel())\n",
    "    counts.index = counts_summary.columns\n",
    "\n",
    "    sorted_vars = counts.sort_values(ascending = False)\n",
    "    ignore = list((sorted_vars[sorted_vars >100]).index)\n",
    "    return ignore\n",
    "\n",
    "def get_data_types(df):\n",
    "    data_types = defaultdict(list)\n",
    "    for entry in df.schema.fields:\n",
    "        data_types[str(entry.dataType)].append(entry.name)\n",
    "    return data_types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(df, ignore):\n",
    "    # Esto lo ponemos aqui para poder modificar las \n",
    "    #variables de los estimadores/transformadores\n",
    "    data_types = get_data_types(df)    \n",
    "    #--------------------------------------\n",
    "    \n",
    "    # -------------- STRING --------------\n",
    "    strings_used = [var for var in data_types[\"StringType\"] if var not in ignore]\n",
    "\n",
    "    # -------------- DOUBLE --------------\n",
    "    numericals_double = [var for var in data_types[\"DoubleType\"] if var not in ignore]\n",
    "    numericals_double_imputed = [var + \"_imputed\" for var in numericals_double]\n",
    "\n",
    "    # -------------- INTEGERS --------------\n",
    "    from pyspark.sql.types import IntegerType, DoubleType\n",
    "    numericals_int = [var for var in data_types[\"IntegerType\"] if var not in ignore]\n",
    "    \n",
    "    for c in numericals_int:\n",
    "        df = df.withColumn(c, df[c].cast(DoubleType()))\n",
    "        df = df.withColumn(c, df[c].cast(\"double\"))\n",
    "        \n",
    "    numericals_int_imputed = [var + \"_imputed\" for var in numericals_int]\n",
    "    # =======================================\n",
    "\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    ##            P I P E L I N E\n",
    "    ## %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "    # ============= ONE HOT ENCODING ================\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "    stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in strings_used]\n",
    "    stage_one_hot = [OneHotEncoder(inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in strings_used]\n",
    "\n",
    "    # =============== IMPUTADORES ====================\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    stage_imputer_double = Imputer(inputCols = numericals_double, \n",
    "                                   outputCols = numericals_double_imputed) \n",
    "    stage_imputer_int = Imputer(inputCols = numericals_int, \n",
    "                                outputCols = numericals_int_imputed) \n",
    "\n",
    "    # ============= VECTOR ASESEMBLER ================\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "    features =  numericals_double_imputed \\\n",
    "              + [var + \"_one_hot\" for var in strings_used]\n",
    "    stage_assembler = VectorAssembler(inputCols = features, outputCol= \"assem_features\")\n",
    "\n",
    "    # ==================== SCALER =======================\n",
    "    from pyspark.ml.feature import StandardScaler\n",
    "    stage_scaler = StandardScaler(inputCol= stage_assembler.getOutputCol(), \n",
    "                                  outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "    # ================== PIPELINE ===================\n",
    "    stages= stage_string + stage_one_hot +  [             # Categorical Data\n",
    "                               stage_imputer_double,\n",
    "                               stage_imputer_int,        # Data Imputation\n",
    "                               stage_assembler,          # Assembling data\n",
    "                               stage_scaler] \n",
    "                          \n",
    "    ## Tenemos que regesar el df porque las variables int las combierte en double\n",
    "    return  stages , df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_params_dic():\n",
    "    stage_pca = PCA(k = 15,inputCol = \"scaled_features\",\n",
    "                        outputCol = \"features\")\n",
    "\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(stage_pca.k, [1]) \\\n",
    "    .addGrid(lr.maxIter, [1]) \\\n",
    "    .build()\n",
    "\n",
    "    dt = DecisionTreeClassifier()\n",
    "\n",
    "    dt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(stage_pca.k, [1]) \\\n",
    "    .addGrid(dt.maxDepth, [2]) \\\n",
    "    .build()\n",
    "\n",
    "    paramGrid_dic= {\"LR\":lr_paramGrid,\"DT\":dt_paramGrid}\n",
    "    model_dic = {\"LR\":lr,\"DT\":dt}\n",
    "\n",
    "    return model_dic,paramGrid_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    data_types = get_data_types(df)\n",
    "    ignore =   ignore_list(df, data_types) \n",
    "    illegal = [s for s in df.columns if \"del\" in s]\n",
    "    extra_illegal = ['cancelled', 'rangoatrasohoras']\n",
    "    legal = [var for var in df.columns if (var not in ignore and var not in illegal and var not in extra_illegal)]\n",
    "    lista_objetivos = df.select('rangoatrasohoras').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "    df = imputa_categoricos(df, ignore,data_types)\n",
    "    \n",
    "    df_legal = df[legal]\n",
    "    y = df[['rangoatrasohoras']]\n",
    "    \n",
    "    df_legal = df_legal.withColumn(\"id\", monotonically_increasing_id())\n",
    "    y = y.withColumn(\"id\", monotonically_increasing_id())\n",
    "    \n",
    "    stages, df_new = create_pipeline(df_legal, ignore)\n",
    "\n",
    "    df_junto = df_new.join(y, \"id\", \"outer\").drop(\"id\")\n",
    "\n",
    "    return df_junto, stages\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def run_model(objetivo, model_name, hyperparams, luigi= False):\n",
    "    df = get_data(luigi)\n",
    "    df, first_stages = prepare_data(df)\n",
    "  \n",
    "    df = df.withColumn(\"label\",  when(df.rangoatrasohoras == objetivo, 1.0).otherwise(0.0))\n",
    "    \n",
    "    # Selecciona el modelo\n",
    "    model_dic, paramGrid_dic  = get_models_params_dic()\n",
    "    clr_model = model_dic[model_name]\n",
    "    \n",
    "    # Parametros especificos\n",
    "    num_it = int(hyperparams[\"iter\"])\n",
    "    if num_it > 0:\n",
    "        clr_model.setMaxIter(num_it)\n",
    "    \n",
    "    # Adds new stages\n",
    "    num_pca = int(hyperparams[\"pca\"])\n",
    "    if num_pca > 0:\n",
    "        stage_pca = PCA(k = num_pca,inputCol = \"scaled_features\", \n",
    "                            outputCol = \"features\")\n",
    "    else:\n",
    "        stage_pca = PCA(k = 8,inputCol = \"scaled_features\", \n",
    "                    outputCol = \"features\")\n",
    "    \n",
    "    # Checar que no se haya corrido este modelo \n",
    "    \n",
    "    print(\"Modelo evaluado: \", clr_model, \"con params: \", clr_model.explainParams())\n",
    "    \n",
    "    # Creates Pipeline\n",
    "    pipeline = Pipeline(stages= first_stages + [stage_pca, clr_model])\n",
    "\n",
    "    df_train, df_test = df.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "    cvModel  = pipeline.fit(df_train)\n",
    "    prediction = cvModel.transform(df_test)\n",
    "    evaluate(prediction)\n",
    "    \n",
    "    #Sacar metadatos\n",
    "    #Insertar metadatos\n",
    "    #Guardar modelos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/src/models\n",
      "Modelo evaluado:  LogisticRegression_da34f1229d49 con params:  aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "Area under ROC = 0.623870\n",
      "Area under PR = 0.017011\n",
      "[[3985.    0.]\n",
      " [  57.    0.]]\n",
      "[Overall]\tprecision = 0.9858980702622464 | recall = 0.9858980702622464 | F1 Measure = 0.9858980702622464\n",
      "[Class 0.0]\tprecision = 0.9858980702622464 | recall = 1.0 | F1 Measure = 0.9886865479085\n",
      "[Class 1.0]\tprecision = 0.0 | recall = 0.0 | F1 Measure = 0.0\n"
     ]
    }
   ],
   "source": [
    "objetivo = \"cancelled\"\n",
    "model = \"LR\"\n",
    "hyperparams = {\"pca\": 1, \"iter\":1}\n",
    "\n",
    "run_model(objetivo, model, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/src/models\n",
      "Modelo evaluado:  LogisticRegression_f943fad70126 con params:  aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "Area under ROC = 0.623870\n",
      "Area under PR = 0.017011\n",
      "[[3985.    0.]\n",
      " [  57.    0.]]\n",
      "[Overall]\tprecision = 0.9858980702622464 | recall = 0.9858980702622464 | F1 Measure = 0.9858980702622464\n",
      "[Class 0.0]\tprecision = 0.9858980702622464 | recall = 1.0 | F1 Measure = 0.9886865479085\n",
      "[Class 1.0]\tprecision = 0.0 | recall = 0.0 | F1 Measure = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUROC': '0.623870',\n",
       " 'AUPR': '0.017011',\n",
       " 'precision': '0.9858980702622464',\n",
       " 'recall': '0.9858980702622464',\n",
       " 'F1 Measure': '0.9858980702622464',\n",
       " 0.0: {'precision': '0.9858980702622464',\n",
       "  'recall': '1.0',\n",
       "  'F1 Measure': '0.9886865479085'},\n",
       " 1.0: {'precision': '0.0', 'recall': '0.0', 'F1 Measure': '0.0'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objetivo = \"cancelled\"\n",
    "model_name = \"LR\"\n",
    "hyperparams = {\"pca\": 1, \"iter\":1}\n",
    "\n",
    "df = get_data(False)\n",
    "df, first_stages = prepare_data(df)\n",
    "\n",
    "df = df.withColumn(\"label\",  when(df.rangoatrasohoras == objetivo, 1.0).otherwise(0.0))\n",
    "\n",
    "# Selecciona el modelo\n",
    "model_dic, paramGrid_dic  = get_models_params_dic()\n",
    "clr_model = model_dic[model_name]\n",
    "\n",
    "# Parametros especificos\n",
    "num_it = int(hyperparams[\"iter\"])\n",
    "if num_it > 0:\n",
    "    clr_model.setMaxIter(num_it)\n",
    "\n",
    "# Adds new stages\n",
    "num_pca = int(hyperparams[\"pca\"])\n",
    "if num_pca > 0:\n",
    "    stage_pca = PCA(k = num_pca,inputCol = \"scaled_features\", \n",
    "                        outputCol = \"features\")\n",
    "else:\n",
    "    stage_pca = PCA(k = 8,inputCol = \"scaled_features\", \n",
    "                outputCol = \"features\")\n",
    "\n",
    "# Checar que no se haya corrido este modelo \n",
    "\n",
    "print(\"Modelo evaluado: \", clr_model, \"con params: \", clr_model.explainParams())\n",
    "\n",
    "# Creates Pipeline\n",
    "pipeline = Pipeline(stages= first_stages + [stage_pca, clr_model])\n",
    "\n",
    "df_train, df_test = df.randomSplit([0.8,0.2], 123)\n",
    "\n",
    "cvModel  = pipeline.fit(df_train)\n",
    "prediction = cvModel.transform(df_test)\n",
    "evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva cubeta\n"
     ]
    }
   ],
   "source": [
    "from src.models.save_model import save_upload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model.zip file is created successfully!\n"
     ]
    }
   ],
   "source": [
    "save_upload(cvModel, objetivo, model_name, hyperparams,bucket_name = \"models-dpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_saved_model = \"./22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model.zip\"\n",
    "folder = new_saved_model[:-4]\n",
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "new_saved_model = \"./22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model.zip\"\n",
    "folder = new_saved_model[:-4]\n",
    "shutil.rmtree(folder, ignore_errors=True)\n",
    "os.remove(new_saved_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(new_saved_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.12.43-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 701 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.16.0,>=1.15.43\n",
      "  Downloading botocore-1.15.43-py2.py3-none-any.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 74 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 47 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3) (1.25.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.43->boto3) (1.14.0)\n",
      "Installing collected packages: docutils, jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.12.43 botocore-1.15.43 docutils-0.15.2 jmespath-0.9.5 s3transfer-0.3.3\n",
      "Nueva cubeta\n",
      "models-dpa\n",
      "models-dpa\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "from src.utils.s3_utils import describe_s3, get_s3_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = cvModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from datetime import date\n",
    "\n",
    "def parse_filename(objetivo, model_name, hyperparams):    \n",
    "    para_string = json.dumps(hyperparams) \n",
    "    para_string = para_string.replace(\" \", \"%\")\n",
    "    para_string = para_string.replace('\"', \"#\")\n",
    "    para_string = para_string.replace('}', \"&\")\n",
    "    para_string = para_string.replace('{', \"=\")\n",
    "    para_string = para_string.replace(':', \"-\")\n",
    "    para_string = para_string.replace(',', \"$\")\n",
    "    \n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%d%m%Y\")\n",
    "\n",
    "    saved_model_name = \"./\" + d1 + \"_\" + objetivo + \"_\" + model_name + \"_\" + para_string \n",
    "\n",
    "    return saved_model_name\n",
    "\n",
    "\n",
    "def save_upload(cvModel, objetivo, model_name, hyperparams,bucket_name = \"models-dpa\"):\n",
    "    trained_model = cvModel.stages[-1]\n",
    "    \n",
    "    saved_model_name = parse_filename(objetivo, model_name, hyperparams) + \".model\"\n",
    "    key_name = saved_model_name[2:]\n",
    "    \n",
    "    # Save model\n",
    "    trained_model.save(saved_model_name)\n",
    "\n",
    "    # Zip model \n",
    "    zip_model(key_name)\n",
    "    \n",
    "    new_saved_model = saved_model_name +\".zip\"\n",
    "    new_key_name = new_saved_model[2:] \n",
    "\n",
    "    # Upload file\n",
    "    upload_file_to_bucket(new_saved_model, bucket_name, new_key_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_and_zip(cvModel, objetivo, model_name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_name = parse_filename(objetivo, model_name, hyperparams)\n",
    "saved_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from src import (\n",
    "    BUCKET,\n",
    "    MY_REGION,\n",
    "    MY_REGION2,\n",
    "    MY_PROFILE,\n",
    "    MY_KEY,\n",
    "    MY_AMI ,\n",
    "    MY_VPC ,\n",
    "    MY_GATEWAY,\n",
    "    MY_SUBNET,\n",
    "    MY_GROUP\n",
    ")\n",
    "\n",
    "\n",
    "ses = boto3.session.Session(profile_name=MY_PROFILE, region_name=MY_REGION,)\n",
    "s3 = ses.resource('s3')\n",
    "bucket_name = BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22042020_cancelled_LR_=#pca#-%1$%#iter#-%1&.model.zip file is created successfully!\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"models-dpa\"\n",
    "#my_bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "model_dir = parse_filename(objetivo, model_name, hyperparams)\n",
    "key_name = saved_model_name[2:]\n",
    "\n",
    "zip_model(key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    " \n",
    "#Declare the function to return all file paths of the particular directory\n",
    "def retrieve_file_paths(dirName):\n",
    "    # setup file paths variable\n",
    "    filePaths = []\n",
    "    # Read all directory, subdirectories and file lists\n",
    "    for root, directories, files in os.walk(dirName):\n",
    "        for filename in files:\n",
    "            # Create the full filepath by using os module.\n",
    "            filePath = os.path.join(root, filename)\n",
    "            filePaths.append(filePath)\n",
    "    # return all paths\n",
    "    return filePaths\n",
    "  \n",
    "def zip_model(dir_name):\n",
    "    #https://linuxhint.com/python_zip_file_directory/\n",
    "    # Call the function to retrieve all files and folders of the assigned directory\n",
    "    filePaths = retrieve_file_paths(dir_name)\n",
    "    # writing files to a zipfile\n",
    "    zip_file = zipfile.ZipFile(dir_name+'.zip', 'w')\n",
    "    with zip_file:\n",
    "        # writing each file one by one\n",
    "        for file in filePaths:\n",
    "            zip_file.write(file)\n",
    "    print(dir_name+'.zip file is created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"models-dpa\"\n",
    "#my_bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "model_dir = parse_filename(objetivo, model_name, hyperparams) +\".zip\"\n",
    "key_name = saved_model_name[2:] \n",
    "\n",
    "ses = boto3.session.Session(profile_name=MY_PROFILE, region_name=MY_REGION,)\n",
    "s3 = ses.resource('s3')\n",
    "\n",
    "# Escribimos el archivo al bucket, usando el binario\n",
    "s3.meta.client.upload_file(model_dir, bucket_name, key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save(saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel, LogisticRegressionModel\n",
    "import os \n",
    "\n",
    "prueba = LogisticRegressionModel.load(saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_2e27ecb108eb, numClasses = 2, numFeatures = 1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CURRENT_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1 = 22042020\n"
     ]
    }
   ],
   "source": [
    "# dd/mm/YY\n",
    "today = date.today()\n",
    "d1 = today.strftime(\"%d%m%Y\")\n",
    "print(\"d1 =\", d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
